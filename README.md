# ğŸ§  Sentiment Analysis with LSTM + Attention

This project implements a sentiment analysis model using a Bidirectional LSTM with an attention mechanism. It is trained on the [Sentiment140](https://www.kaggle.com/datasets/kazanova/sentiment140) dataset, which contains 1.6 million tweets labeled as positive or negative.

The goal is to explore how attention mechanisms can enhance text classification by allowing the model to focus on the most relevant parts of an input sequence.

---

## ğŸ“‚ Project Structure

text-classification-attention/

â”œâ”€â”€ data/ # Contains the Sentiment140 dataset

â”œâ”€â”€ model/

â”‚ â”œâ”€â”€ attention_lstm.py # Model definition with attention

â”‚ â””â”€â”€ saved_models/ # Trained model (.h5), tokenizer, and training plot

â”œâ”€â”€ utils/

â”‚ â””â”€â”€ preprocess.py # Text preprocessing and tokenization

â”œâ”€â”€ train.py # Script to train and save the model

â”œâ”€â”€ visualize_attention.py # (optional) Visualization script for attention

â”œâ”€â”€ requirements.txt

â””â”€â”€ README.md

-----

## ğŸ› ï¸ Installation

# Create and activate a virtual environment (recommended)
conda create -n attention-nlp python=3.9
conda activate attention-nlp

# Install dependencies
pip install -r requirements.txt

---

## ğŸ§ª Training
python train.py

The script will:

Preprocess and tokenize the text

Train the model with early stopping

Save the model (.h5) and tokenizer (.pkl)

Plot training history

---

## ğŸ” Attention Visualization (Optional)
To inspect what the model focuses on in a sentence, you can run:
python visualize_attention.py
You can modify the sentence variable in the script to try different inputs.

---

## ğŸ“Š Results
The model uses:

Embedding layer with masking

Bidirectional LSTM (64 units)

Custom attention mechanism

Dense output layer for binary classification

Training/validation accuracy and loss are plotted in model/saved_models/training_history.png.

---

## ğŸ“˜ Dataset Info
Source: Sentiment140 - Kaggle

Size: 1.6 million tweets

Labels:

0: Negative sentiment

4: Positive sentiment

Labels were converted to 0 (negative) and 1 (positive) for binary classification.

## âœ¨ Motivation
This project was developed as a personal learning initiative to better understand attention mechanisms in Natural Language Processing (NLP), particularly how they enhance interpretability and improve performance in sentiment classification tasks involving noisy or informal text like tweets.

---

ğŸ§  Credits
Built with â¤ï¸ by Sirine Hjaij â€” Based on "Sujet 13: MÃ©canisme d'attention appliquÃ© Ã  la classification de texte".
